{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14444752,"sourceType":"datasetVersion","datasetId":9226809}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install lightning transformers torchaudio nnAudio scikit-learn pandas tqdm"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T12:25:33.199649Z","iopub.execute_input":"2026-01-09T12:25:33.199837Z","iopub.status.idle":"2026-01-09T12:25:38.915050Z","shell.execute_reply.started":"2026-01-09T12:25:33.199817Z","shell.execute_reply":"2026-01-09T12:25:38.914218Z"},"id":"s_yYA4vMx00L","outputId":"de9df360-4969-435b-d64d-2f183ab27ee1"},"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.6.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nCollecting nnAudio\n  Downloading nnaudio-0.3.4-py3-none-any.whl.metadata (771 bytes)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\nRequirement already satisfied: PyYAML<8.0,>5.4 in /usr/local/lib/python3.12/dist-packages (from lightning) (6.0.3)\nRequirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (2025.10.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (0.15.2)\nRequirement already satisfied: packaging<27.0,>=20.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (25.0)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (2.8.0+cu126)\nRequirement already satisfied: torchmetrics<3.0,>0.7.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (1.8.2)\nRequirement already satisfied: typing-extensions<6.0,>4.5.0 in /usr/local/lib/python3.12/dist-packages (from lightning) (4.15.0)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.12/dist-packages (from lightning) (2.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.0)\nRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from nnAudio) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning) (3.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.1rc0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning) (1.22.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.3)\nDownloading lightning-2.6.0-py3-none-any.whl (845 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m846.0/846.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nnaudio-0.3.4-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nnAudio, lightning\nSuccessfully installed lightning-2.6.0 nnAudio-0.3.4\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","\n","import lightning as L\n","\n","from transformers import AutoModel, Wav2Vec2FeatureExtractor\n","\n","from typing import List, Tuple, Dict, Any, Union, Optional\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n","        self.bn1 = nn.BatchNorm1d(out_channels)\n","        self.relu = nn.ReLU()\n","        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding)\n","        self.bn2 = nn.BatchNorm1d(out_channels)\n","        self.shortcut = nn.Sequential()\n","        if in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n","                nn.BatchNorm1d(out_channels)\n","            )\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out += self.shortcut(x)\n","        out = self.relu(out)\n","        return out\n","\n","\n","class SiameseNet(nn.Module):\n","    def __init__(self, embedding_dim: int):\n","        super(SiameseNet, self).__init__()\n","        self.layer1 = ResidualBlock(3072, 512)\n","        self.layer2 = ResidualBlock(512, 256)\n","        self.global_pool = nn.AdaptiveAvgPool1d(1)\n","        self.fc = nn.Linear(256, embedding_dim)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.global_pool(x)\n","        x = x.view(x.size(0), -1)  # Flatten the tensor\n","        x = self.fc(x)\n","        return x\n","\n","    def similarity_score(self, sample1, sample2, metric='euclidean'):\n","        if metric == 'euclidean':\n","            return torch.nn.functional.pairwise_distance(sample1, sample2)\n","        elif metric == 'cosine':\n","            return 1 - torch.nn.functional.cosine_similarity(sample1, sample2)\n","\n","class PlagiarismDetectionSystem(L.LightningModule):\n","    def __init__(self, config: Dict):\n","        super().__init__()\n","        train_classifier_gap = config[\"train_classifier_gap\"]\n","        embedding_dim = config[\"siamese_emb_dim\"]\n","\n","        # Feature extractor trained by triplet loss\n","        self.siamese_net = SiameseNet(embedding_dim=embedding_dim)\n","\n","        # Classification head\n","        self.classifier = nn.Sequential(\n","            nn.Linear(embedding_dim, embedding_dim),\n","            nn.ReLU(),\n","            nn.Linear(embedding_dim, embedding_dim),\n","            nn.ReLU(),\n","            nn.Linear(embedding_dim, 1),\n","        )\n","\n","        # loss functions\n","        self.criterion_triplet = nn.TripletMarginLoss(margin=2, p=2)\n","        self.criterion_classification = nn.BCEWithLogitsLoss()\n","\n","        # how often to train the classification head\n","        self.train_classifier_gap = train_classifier_gap\n","\n","        # audio model for inference\n","        if not hasattr(self, \"audio_processor\"):\n","            self.audio_processor = Wav2Vec2FeatureExtractor.from_pretrained(\"m-a-p/MERT-v1-95M\")\n","        if not hasattr(self, \"audio_model\"):\n","            self.audio_model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-95M\", trust_remote_code=True).to(self.device)\n","\n","    def forward_siamese_net(self, anchors, positives, negatives):\n","        triplet_embeddings = torch.stack([\n","                self.siamese_net(anchors),\n","                self.siamese_net(positives),\n","                self.siamese_net(negatives)\n","            ], dim=1\n","        )\n","        return triplet_embeddings\n","\n","    def forward_classifier(self, triplet_embeddings):\n","        diff_same = torch.abs(triplet_embeddings[:,0] - triplet_embeddings[:,1])\n","        diff_diff = torch.abs(triplet_embeddings[:,0] - triplet_embeddings[:,2])\n","        logit_same = self.classifier(diff_same).squeeze()\n","        logit_diff = self.classifier(diff_diff).squeeze()\n","        return logit_same, logit_diff\n","\n","    def training_step(self, batch, batch_idx):\n","        anchors, positives, negatives = batch\n","        B = anchors.shape[0]\n","\n","        # train siamese net\n","        triplet_embeddings = self.forward_siamese_net(anchors, positives, negatives)\n","        loss_triplet = self.criterion_triplet(\n","            triplet_embeddings[:,0], triplet_embeddings[:,1], triplet_embeddings[:,2]\n","        ) # anchor_embeddings, positive_embeddings, negative_embeddings\n","\n","        # train classifier\n","        train_classifier = False\n","        if self.train_classifier_gap is None:\n","            train_classifier = True\n","        elif self.global_step // self.train_classifier_gap == self.train_classifier_gap - 1:\n","            train_classifier = True\n","\n","        if train_classifier:\n","            labels_same = torch.zeros(B).to(triplet_embeddings.device).float()\n","            labels_diff = torch.ones(B).to(triplet_embeddings.device).float()\n","            logit_same, logit_diff = self.forward_classifier(triplet_embeddings.detach())\n","            loss_same = self.criterion_classification(logit_same, labels_same.squeeze())\n","            loss_diff = self.criterion_classification(logit_diff, labels_diff.squeeze())\n","            loss_classification = (loss_same + loss_diff) / 2\n","        else:\n","            loss_classification = 0\n","\n","        # final loss\n","        final_loss = loss_triplet + loss_classification\n","        self.log(\"triplet_loss\", loss_triplet)\n","        self.log(\"classification_loss\", loss_classification)\n","        self.log(\"total_loss\", final_loss)\n","        return final_loss\n","\n","    @torch.no_grad()\n","    def validation_step(self, batch, batch_idx):\n","        # B, three = batch.shape[0], batch.shape[1]\n","        # assert three == 3\n","        anchors, positives, negatives = batch\n","        B = anchors.shape[0]\n","\n","        triplet_embeddings = self.forward_siamese_net(anchors, positives, negatives)\n","        loss_triplet = self.criterion_triplet(\n","            triplet_embeddings[:,0], triplet_embeddings[:,1], triplet_embeddings[:,2]\n","        ).cpu().item() # anchor_embeddings, positive_embeddings, negative_embeddings\n","\n","        labels_same = torch.zeros(B).to(triplet_embeddings.device)\n","        labels_diff = torch.ones(B).to(triplet_embeddings.device)\n","        logit_same, logit_diff = self.forward_classifier(triplet_embeddings)\n","        loss_same = self.criterion_classification(logit_same, labels_same)\n","        loss_diff = self.criterion_classification(logit_diff, labels_diff)\n","        loss_classification = ((loss_same + loss_diff) / 2).cpu().item()\n","\n","        # \">\": normal decision (different song, large logit -> TRUE; same song, small logit -> FALSE)\n","        # If using \"<\", then it is reverting the decision\n","\n","        preds_same = torch.sigmoid(logit_same) > 0.5\n","        preds_diff = torch.sigmoid(logit_diff) > 0.5\n","        # preds_same = self._inference_step(batch[:,0], batch[:,1]) > 0.5 # same operation\n","        # preds_diff = self._inference_step(batch[:,0], batch[:,2]) > 0.5\n","        preds = torch.cat([preds_same, preds_diff])\n","        labels = torch.cat([labels_same, labels_diff])\n","\n","        accuracy = (preds.cpu() == labels.cpu()).float().mean()  # Batch accuracy = overall accuracy when batch_size = dataset_size\n","        accuracy_positive = (preds[:B].cpu() == labels_same.cpu()).float().mean()\n","\n","        self.log(\"val_triplet_loss\", loss_triplet, prog_bar=True)\n","        self.log(\"val_classification_loss\", loss_classification, prog_bar=True)\n","        self.log(\"val_accuracy\", accuracy, prog_bar=True)\n","        self.log(\"val_accuracy_positive\", accuracy, prog_bar=True)\n","\n","        return {\n","            \"val_triplet_loss\": loss_triplet,\n","            \"val_classification_loss\": loss_classification,\n","        }\n","\n","    @torch.no_grad()\n","    def _inference_step(self, sample1:torch.Tensor, sample2:torch.Tensor):\n","        B1 = sample1.shape[0]\n","        B2 = sample2.shape[0]\n","        assert B1 == B2\n","        B = B1\n","\n","        out_embs1 = self.siamese_net(sample1)\n","        out_embs2 = self.siamese_net(sample2)\n","        diff = torch.abs(out_embs1 - out_embs2)\n","        logit = self.classifier(diff).squeeze()\n","        scores = torch.sigmoid(logit)\n","        return scores\n","\n","    @torch.no_grad()\n","    def inference_pairs(\n","        self,\n","        waveforms1:Union[List[torch.Tensor], torch.Tensor],\n","        waveforms2:Union[List[torch.Tensor], torch.Tensor],\n","    ):\n","        time_reduce = torch.nn.AvgPool1d(kernel_size=10, stride=10, count_include_pad=False).to(self.device)\n","        self.eval()\n","\n","        if type(waveforms1) == list and type(waveforms2) == list:\n","            assert len(waveforms1) == len(waveforms2)\n","            waveforms1 = torch.stack(waveforms1).to(self.device)\n","            waveforms2 = torch.stack(waveforms2).to(self.device)\n","        elif torch.is_tensor(waveforms1) and torch.is_tensor(waveforms2):\n","            assert waveforms1.shape[0] == waveforms2.shape[0]\n","            assert waveforms1.dim() == 2 and waveforms2.dim() == 2\n","        else:\n","            assert 0\n","\n","        # extract MERT features\n","        hidden_states1 = self.audio_model(waveforms1, output_hidden_states=True).hidden_states\n","        hidden_states2 = self.audio_model(waveforms2, output_hidden_states=True).hidden_states\n","        mert_features1 = torch.stack(\n","            [time_reduce(h.detach()[:, :, :].permute(0,2,1)).permute(0,2,1) for h in hidden_states1[2::3]], dim=1\n","        )\n","        mert_features2 = torch.stack(\n","            [time_reduce(h.detach()[:, :, :].permute(0,2,1)).permute(0,2,1) for h in hidden_states2[2::3]], dim=1\n","        )\n","        batch_num, num_layers, num_frames, layer_dim = mert_features1.shape\n","        mert_features1 = mert_features1.permute(0, 1, 3, 2) # [batch_num, num_layers=4, layer_dim=768, num_frames]\n","        mert_features2 = mert_features2.permute(0, 1, 3, 2) # [batch_num, num_layers=4, layer_dim=768, num_frames]\n","        assert mert_features1.shape[1] == 4 and mert_features1.shape[2] == 768\n","        # mert_features = mert_features.reshape(batch_num, num_layers * layer_dim, num_frames)\n","        mert_features1 = torch.cat([mert_features1[:,i] for i in range(mert_features1.shape[1])], dim=1)\n","        mert_features2 = torch.cat([mert_features2[:,i] for i in range(mert_features2.shape[1])], dim=1)\n","\n","        # get scores for decisions\n","        # num_features = mert_features.shape[0] // 2\n","        scores = self._inference_step(mert_features1, mert_features2)\n","\n","        return 1 - scores # similarity, the higher the more similar (distance smaller)\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.Adam(\n","            list(self.siamese_net.parameters()) + list(self.classifier.parameters()),\n","            lr=1e-3\n","        )\n","        return optimizer\n","\n","\n","print(\"‚úÖ System Classes Defined\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T12:43:21.878430Z","iopub.execute_input":"2026-01-09T12:43:21.878966Z","iopub.status.idle":"2026-01-09T12:43:21.907436Z","shell.execute_reply.started":"2026-01-09T12:43:21.878935Z","shell.execute_reply":"2026-01-09T12:43:21.906750Z"},"id":"QrIifGBtx00O","outputId":"c32c54df-e20e-4f43-8b8d-4412baa5fcfd"},"outputs":[{"name":"stdout","text":"‚úÖ System Classes Defined\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import os\n","import random\n","from sklearn import metrics\n","from tqdm import tqdm\n","import glob\n","\n","config = {\n","    \"siamese_emb_dim\": 128,\n","    \"train_classifier_gap\": None\n","}\n","\n","def final_kaggle_evaluation():\n","    print(\"üöÄ Starting Evaluation (Robust Tensor Reshape)...\")\n","\n","    # Setup\n","    model_files = glob.glob(\"/kaggle/input/**/*.ckpt\", recursive=True)\n","    if not model_files: return\n","    checkpoint_path = model_files[0]\n","\n","    track_files = glob.glob(\"/kaggle/input/**/*.npy\", recursive=True)\n","    if not track_files: return\n","\n","    try:\n","        track_files.sort(key=lambda x: int(os.path.basename(x).replace('pair_', '').replace('.npy', '')))\n","    except: pass\n","\n","    # Load Model\n","    print(f\"üì• Loading Model: {checkpoint_path}\")\n","    model = PlagiarismDetectionSystem.load_from_checkpoint(checkpoint_path, config=config, map_location='cuda', strict=False)\n","    model.eval().cuda()\n","\n","    # Load Data\n","    all_track_data = [np.load(f) for f in track_files]\n","\n","    #  Pairs\n","    positive_pairs = []\n","    negative_pairs = []\n","    num_tracks = len(all_track_data)\n","\n","    for i in range(num_tracks):\n","        if all_track_data[i].shape[0] >= 2:\n","            positive_pairs.append((i, 0, i, 1))\n","\n","    valid_indices = [i for i in range(num_tracks) if all_track_data[i].shape[0] >= 1]\n","    while len(negative_pairs) < len(positive_pairs):\n","        idx1 = random.choice(valid_indices)\n","        idx2 = random.choice(valid_indices)\n","        if idx1 != idx2:\n","            negative_pairs.append((idx1, 0, idx2, 0))\n","\n","    all_pairs = positive_pairs + negative_pairs\n","    all_labels = [1] * len(positive_pairs) + [0] * len(negative_pairs)\n","\n","    print(f\"‚öñÔ∏è Evaluating on {len(all_pairs)} pairs...\")\n","\n","    # Distances\n","    print(\"Computing Distances...\")\n","    best_acc = 0\n","    best_thresh = 0\n","    best_preds = []\n","    distances = []\n","\n","    for t1, v1, t2, v2 in tqdm(all_pairs):\n","        seg1 = all_track_data[t1][v1]\n","        seg2 = all_track_data[t2][v2]\n","\n","        # Robust Prepare Tensor\n","        def prepare_tensor(seg):\n","            t = torch.from_numpy(seg).float().cuda()\n","\n","            if t.ndim == 4:\n","                # Segments: (Seg, Time, Layers, Feat)\n","                t = t.permute(0, 2, 1, 3)\n","                # Segments * Time -> Total Time\n","                # Layers * Feat -> Total Channels (3072)\n","                # Shape: (Total_Time, 4, 768)\n","                t = t.reshape(-1, 4, 768)\n","                #    Shape: (Total_Time, 3072)\n","                t = t.reshape(t.shape[0], -1)\n","                t = t.transpose(0, 1) # -> (3072, Total_Time)\n","\n","            # Simple Case: (Layers, Time, Feat) -> [4, T, 768]\n","            elif t.ndim == 3 and t.shape[0] == 4 and t.shape[2] == 768:\n","                t = t.permute(0, 2, 1) # (4, 768, T)\n","                t = t.reshape(-1, t.shape[-1]) # (3072, T)\n","\n","            if t.shape[0] != 3072 and t.shape[1] == 3072:\n","                t = t.transpose(0, 1)\n","\n","            return t.unsqueeze(0)\n","\n","        input1 = prepare_tensor(seg1)\n","        input2 = prepare_tensor(seg2)\n","\n","        with torch.no_grad():\n","            emb1 = model.siamese_net(input1)\n","            emb2 = model.siamese_net(input2)\n","            dist = torch.dist(emb1, emb2, p=2).item()\n","            distances.append(dist)\n","\n","    # Tuning\n","    print(\"\\nTuning Threshold...\")\n","    max_dist = max(distances) if distances else 1.0\n","    thresholds = np.linspace(0, max_dist, 100)\n","\n","    for thresh in thresholds:\n","        preds = [1 if d < thresh else 0 for d in distances]\n","        acc = metrics.accuracy_score(all_labels, preds)\n","        if acc > best_acc:\n","            best_acc = acc\n","            best_thresh = thresh\n","            best_preds = preds\n","\n","    print(\"\\n\" + \"=\"*40)\n","    print(f\"üèÜ KAGGLE FINAL RESULT\")\n","    print(f\"‚öôÔ∏è Best Distance Threshold: < {best_thresh:.4f}\")\n","    print(\"=\"*40)\n","    print(metrics.classification_report(all_labels, best_preds, target_names=[\"Different\", \"Similar/Plagiarism\"]))\n","    print(f\"‚úÖ Max Accuracy: {best_acc:.2%}\")\n","    print(f\"Confusion Matrix:\\n{metrics.confusion_matrix(all_labels, best_preds)}\")\n","\n","final_kaggle_evaluation()"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T12:48:34.149790Z","iopub.execute_input":"2026-01-09T12:48:34.150393Z","iopub.status.idle":"2026-01-09T12:48:38.329430Z","shell.execute_reply.started":"2026-01-09T12:48:34.150358Z","shell.execute_reply":"2026-01-09T12:48:38.328713Z"},"id":"nhcu_bB5x00P","outputId":"32816b89-f77e-4707-e304-e58d6143de7b"},"outputs":[{"name":"stdout","text":"üöÄ Starting Evaluation (Robust Tensor Reshape)...\nüì• Loading Model: /kaggle/input/thesis-complete/best_model_continued-epoch7.ckpt\n‚öñÔ∏è Evaluating on 96 pairs...\nComputing Distances...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [00:01<00:00, 53.36it/s]","output_type":"stream"},{"name":"stdout","text":"\nTuning Threshold...\n\n========================================\nüèÜ KAGGLE FINAL RESULT\n‚öôÔ∏è Best Distance Threshold: < 1.1499\n========================================\n                    precision    recall  f1-score   support\n\n         Different       0.79      0.79      0.79        48\nSimilar/Plagiarism       0.79      0.79      0.79        48\n\n          accuracy                           0.79        96\n         macro avg       0.79      0.79      0.79        96\n      weighted avg       0.79      0.79      0.79        96\n\n‚úÖ Max Accuracy: 79.17%\nConfusion Matrix:\n[[38 10]\n [10 38]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":null}]}